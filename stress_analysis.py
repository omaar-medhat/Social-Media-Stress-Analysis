# -*- coding: utf-8 -*-
"""Stress Analysis.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1oOicOUGKdd9HFQ1F8PhWxT-qFqpYgv2n
"""

# Commented out IPython magic to ensure Python compatibility.
import numpy as np
from matplotlib import pyplot as plt
# %matplotlib inline
import pandas as pd
import requests as rq
from bs4 import BeautifulSoup as bs
!pip install google-search-results
from serpapi import GoogleSearch
!pip install networkx
import networkx as nt
!pip install opencv-python
import cv2
import networkx

params={
    "q":"reddit Dataset for Stress Analysis in Social Media",
    "engine":"google",
    "api_key":"32c55d0aa4f94b3d91e1fc1626a469489097a8c31671f40afb8fd8488879c9f7",
}
search=GoogleSearch(params)
results=search.get_dict()
# print(results)
res=results["organic_results"]
# print(res)
find_kaggle=""
for i in res:
  if "kaggle.com" in i ["link"]:
    find_kaggle=i
    print(find_kaggle)

kaggle_url=find_kaggle["link"]
kaggle_respone=rq.get(kaggle_url)
print(kaggle_respone)
kaggle_respone.content

soup=bs(kaggle_respone.content,"html5lib")
print(soup)

links = soup.select('link')
links

find_url=""
for i in res:
  if "dataset" in i ["link"]:
    find_url=i
    print(find_url)

!unzip "/content/archive.zip"

df=pd.read_csv("dreaddit-train.csv")
df2=pd.DataFrame(df)
# df2.to_csv("fram.csv")
print(df2)
print("=============================")
print(df2.columns)
print("=============================")
print(df2.shape)

subreddit=df['subreddit'].value_counts()
plt.figure(figsize=(10,5))
subreddit.plot(kind='bar')
plt.title('subreddit column')
plt.xlabel('subreddit')
plt.ylabel('count')
plt.show()

label=df['label'].value_counts()
plt.figure(figsize=(10, 5))
label.plot(kind='bar')
plt.title('label column')
plt.xlabel('Label')
plt.ylabel('count')
plt.show()

plt.figure(figsize=(10, 5))
plt.scatter(df['social_num_comments'],df["social_upvote_ratio"])
plt.title("relationship between social_num_comments column and social_upvote_ratio")
plt.xlabel("social_num_comments")
plt.ylabel("social_upvote_ratio")
plt.show()

first_list_words=["anxious","stressed","worried"]
first_list_count=[]
for i in range(len(first_list_words)):
  count=0
  for j in range(len(df)):
    if df.loc[j]["text"].find(first_list_words[i])!=-1:
      count+=1
  first_list_count.append(count)
print(first_list_count)

second_list_words=["relaxed","calm","happy"]
second_list_count=[]
for u in range(len(second_list_words)):
  count2=0
  for k in range(len(df)):
    if df.loc[k]["text"].find(second_list_words[u])!=-1:
      count2+=1
  second_list_count.append(count2)
print(second_list_count)

texts = df.head(2)["text"]
print(texts)
string = ""
for text in texts:
  string += text
words = string.split()
counts = {}
for word in words:
    if word in counts:
        counts[word] += 1
    else:
        counts[word] = 1
for word, count in counts.items():
    print(f"{word}: {count}")

top_words = list(counts.keys())[:20]
g =nt.Graph()
g.add_nodes_from(top_words)
print(g.nodes)

data_array = df.to_numpy()
x = data_array[:,df.columns.get_loc('confidence'):]
y = data_array[:, df.columns.get_loc('label'):]
print("x shape is: ",x.shape)
print("y shape is: ",y.shape)
print("")
print("")
print("Data Sample:")
print("")
print("data sample for x: ", x[0])
print("")
print("data sample for y: ",y[0])